{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<gluonnlp.data.dataset.TSVDataset at 0x7fd8980d54a8>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)# <-- return_dict=False 추가했습니다\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/nlplab/anaconda3/envs/momoKoBERT/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  \"\"\"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e278bd01a4744494a3684a3aa3cdc185"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 batch id 1 loss 0.7446473836898804 train acc 0.453125\n",
      "epoch 1 train acc 0.453125\n",
      "epoch 1 batch id 201 loss 0.503562867641449 train acc 0.5498289800995025\n",
      "epoch 1 train acc 0.5498289800995025\n",
      "epoch 1 batch id 401 loss 0.40499842166900635 train acc 0.6698488154613467\n",
      "epoch 1 train acc 0.6698488154613467\n",
      "epoch 1 batch id 601 loss 0.3958743214607239 train acc 0.7235076955074875\n",
      "epoch 1 train acc 0.7235076955074875\n",
      "epoch 1 batch id 801 loss 0.40475329756736755 train acc 0.754603620474407\n",
      "epoch 1 train acc 0.754603620474407\n",
      "epoch 1 batch id 1001 loss 0.27074697613716125 train acc 0.7739760239760239\n",
      "epoch 1 train acc 0.7739760239760239\n",
      "epoch 1 batch id 1201 loss 0.3654276430606842 train acc 0.7872085761865112\n",
      "epoch 1 train acc 0.7872085761865112\n",
      "epoch 1 batch id 1401 loss 0.29545465111732483 train acc 0.7969196109921485\n",
      "epoch 1 train acc 0.7969196109921485\n",
      "epoch 1 batch id 1601 loss 0.38534975051879883 train acc 0.8057659275452842\n",
      "epoch 1 train acc 0.8057659275452842\n",
      "epoch 1 batch id 1801 loss 0.2537241280078888 train acc 0.8124479455857857\n",
      "epoch 1 train acc 0.8124479455857857\n",
      "epoch 1 batch id 2001 loss 0.302310973405838 train acc 0.8188249625187406\n",
      "epoch 1 train acc 0.8188249625187406\n",
      "epoch 1 batch id 2201 loss 0.30878037214279175 train acc 0.8238016810540664\n",
      "epoch 1 train acc 0.8238016810540664\n",
      "/home/nlplab/anaconda3/envs/momoKoBERT/lib/python3.6/site-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "596fb84750d94a9b91455a347f62414b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1 test acc 0.8824928069053708\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0932a914eb504048a7647d44ac5cf1ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 2 batch id 1 loss 0.48628661036491394 train acc 0.828125\n",
      "epoch 2 train acc 0.828125\n",
      "epoch 2 batch id 201 loss 0.18765553832054138 train acc 0.8823849502487562\n",
      "epoch 2 train acc 0.8823849502487562\n",
      "epoch 2 batch id 401 loss 0.31057247519493103 train acc 0.8836502493765586\n",
      "epoch 2 train acc 0.8836502493765586\n",
      "epoch 2 batch id 601 loss 0.3861555755138397 train acc 0.8874532029950083\n",
      "epoch 2 train acc 0.8874532029950083\n",
      "epoch 2 batch id 801 loss 0.39910972118377686 train acc 0.8895911360799001\n",
      "epoch 2 train acc 0.8895911360799001\n",
      "epoch 2 batch id 1001 loss 0.2868378758430481 train acc 0.8914835164835165\n",
      "epoch 2 train acc 0.8914835164835165\n",
      "epoch 2 batch id 1201 loss 0.2063954621553421 train acc 0.8935132181515404\n",
      "epoch 2 train acc 0.8935132181515404\n",
      "epoch 2 batch id 1401 loss 0.19702255725860596 train acc 0.8962236795146324\n",
      "epoch 2 train acc 0.8962236795146324\n",
      "epoch 2 batch id 1601 loss 0.32333388924598694 train acc 0.8980227201748907\n",
      "epoch 2 train acc 0.8980227201748907\n",
      "epoch 2 batch id 1801 loss 0.1784844845533371 train acc 0.8998212798445309\n",
      "epoch 2 train acc 0.8998212798445309\n",
      "epoch 2 batch id 2001 loss 0.25863057374954224 train acc 0.90163511994003\n",
      "epoch 2 train acc 0.90163511994003\n",
      "epoch 2 batch id 2201 loss 0.2511674463748932 train acc 0.9032187074057246\n",
      "epoch 2 train acc 0.9032187074057246\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b43e8a91acc746e2b68ec976d5c356d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 2 test acc 0.890005594629156\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "568fc74aa0424f8bb9e24b6cbb2fc989"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 3 batch id 1 loss 0.46752652525901794 train acc 0.828125\n",
      "epoch 3 train acc 0.828125\n",
      "epoch 3 batch id 201 loss 0.06954506039619446 train acc 0.923818407960199\n",
      "epoch 3 train acc 0.923818407960199\n",
      "epoch 3 batch id 401 loss 0.221136212348938 train acc 0.9270963216957606\n",
      "epoch 3 train acc 0.9270963216957606\n",
      "epoch 3 batch id 601 loss 0.33099624514579773 train acc 0.9286605657237936\n",
      "epoch 3 train acc 0.9286605657237936\n",
      "epoch 3 batch id 801 loss 0.2513745427131653 train acc 0.9307311173533084\n",
      "epoch 3 train acc 0.9307311173533084\n",
      "epoch 3 batch id 1001 loss 0.1734064817428589 train acc 0.9323489010989011\n",
      "epoch 3 train acc 0.9323489010989011\n",
      "epoch 3 batch id 1201 loss 0.1946326643228531 train acc 0.9342084721065779\n",
      "epoch 3 train acc 0.9342084721065779\n",
      "epoch 3 batch id 1401 loss 0.10017234086990356 train acc 0.9363958779443254\n",
      "epoch 3 train acc 0.9363958779443254\n",
      "epoch 3 batch id 1601 loss 0.20621083676815033 train acc 0.937373126171143\n",
      "epoch 3 train acc 0.937373126171143\n",
      "epoch 3 batch id 1801 loss 0.09461537003517151 train acc 0.9388967934480844\n",
      "epoch 3 train acc 0.9388967934480844\n",
      "epoch 3 batch id 2001 loss 0.20354363322257996 train acc 0.9399909420289855\n",
      "epoch 3 train acc 0.9399909420289855\n",
      "epoch 3 batch id 2201 loss 0.13674713671207428 train acc 0.9408720467969105\n",
      "epoch 3 train acc 0.9408720467969105\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21ed57f37066468899cbaeb956163afc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 3 test acc 0.8952006074168798\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fd3af906d284e1791a4fbb79b70769c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 4 batch id 1 loss 0.3706313967704773 train acc 0.890625\n",
      "epoch 4 train acc 0.890625\n",
      "epoch 4 batch id 201 loss 0.05133739858865738 train acc 0.9561567164179104\n",
      "epoch 4 train acc 0.9561567164179104\n",
      "epoch 4 batch id 401 loss 0.12500928342342377 train acc 0.9559304862842892\n",
      "epoch 4 train acc 0.9559304862842892\n",
      "epoch 4 batch id 601 loss 0.1560756117105484 train acc 0.9574147254575707\n",
      "epoch 4 train acc 0.9574147254575707\n",
      "epoch 4 batch id 801 loss 0.1701204627752304 train acc 0.9592111423220974\n",
      "epoch 4 train acc 0.9592111423220974\n",
      "epoch 4 batch id 1001 loss 0.014644648879766464 train acc 0.960320929070929\n",
      "epoch 4 train acc 0.960320929070929\n",
      "epoch 4 batch id 1201 loss 0.033677101135253906 train acc 0.9614253746877602\n",
      "epoch 4 train acc 0.9614253746877602\n",
      "epoch 4 batch id 1401 loss 0.03913336992263794 train acc 0.9624152391149179\n",
      "epoch 4 train acc 0.9624152391149179\n",
      "epoch 4 batch id 1601 loss 0.10642174631357193 train acc 0.963216349156777\n",
      "epoch 4 train acc 0.963216349156777\n",
      "epoch 4 batch id 1801 loss 0.06836295872926712 train acc 0.9641258328706275\n",
      "epoch 4 train acc 0.9641258328706275\n",
      "epoch 4 batch id 2001 loss 0.13901454210281372 train acc 0.9649003623188406\n",
      "epoch 4 train acc 0.9649003623188406\n",
      "epoch 4 batch id 2201 loss 0.08195383846759796 train acc 0.965363755111313\n",
      "epoch 4 train acc 0.965363755111313\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cf8101622634cc690657a4481246074"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 4 test acc 0.8975583439897699\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1209884aa5d4ed2999d5090a83cf73c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 5 batch id 1 loss 0.38297325372695923 train acc 0.875\n",
      "epoch 5 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.050985872745513916 train acc 0.9743470149253731\n",
      "epoch 5 train acc 0.9743470149253731\n",
      "epoch 5 batch id 401 loss 0.05175464227795601 train acc 0.9754909600997507\n",
      "epoch 5 train acc 0.9754909600997507\n",
      "epoch 5 batch id 601 loss 0.26742425560951233 train acc 0.9756135607321131\n",
      "epoch 5 train acc 0.9756135607321131\n",
      "epoch 5 batch id 801 loss 0.10472835600376129 train acc 0.9765332397003745\n",
      "epoch 5 train acc 0.9765332397003745\n",
      "epoch 5 batch id 1001 loss 0.005985351745039225 train acc 0.9771634615384616\n",
      "epoch 5 train acc 0.9771634615384616\n",
      "epoch 5 batch id 1201 loss 0.04447946324944496 train acc 0.9773235845129059\n",
      "epoch 5 train acc 0.9773235845129059\n",
      "epoch 5 batch id 1401 loss 0.014704975299537182 train acc 0.9774379907209136\n",
      "epoch 5 train acc 0.9774379907209136\n",
      "epoch 5 batch id 1601 loss 0.012505337595939636 train acc 0.9778556371018113\n",
      "epoch 5 train acc 0.9778556371018113\n",
      "epoch 5 batch id 1801 loss 0.007234151475131512 train acc 0.9781892004441977\n",
      "epoch 5 train acc 0.9781892004441977\n",
      "epoch 5 batch id 2001 loss 0.07623527944087982 train acc 0.9785263618190905\n",
      "epoch 5 train acc 0.9785263618190905\n",
      "epoch 5 batch id 2201 loss 0.037847913801670074 train acc 0.9786318718764198\n",
      "epoch 5 train acc 0.9786318718764198\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2bf8e68ac0c492ea3c319f16c50c102"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 5 test acc 0.8985174232736572\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "source": [
    "#todo\n",
    "\n",
    "다른 데이터셋들도 돌려보고 버트 이해하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('momoKoBERT': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "fdec29f9b47567deea31828944d53754aeb926d3c1b9335ea3443d6e8a8e588c"
   }
  },
  "interpreter": {
   "hash": "fdec29f9b47567deea31828944d53754aeb926d3c1b9335ea3443d6e8a8e588c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}